/global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, "__version__") or LooseVersion(
wandb: Currently logged in as: doseok (social-game-rl). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.2
wandb: Run data is saved locally in /global/home/users/yanlarry/ActiveRL/wandb/run-20221015_141843-gaohb5qn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-deluge-138
wandb: ‚≠êÔ∏è View project at https://wandb.ai/social-game-rl/active-rl
wandb: üöÄ View run at https://wandb.ai/social-game-rl/active-rl/runs/gaohb5qn
/global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/wandb/sdk/lib/import_hooks.py:246: DeprecationWarning: Deprecated since Python 3.4 and slated for removal in Python 3.12; use importlib.util.find_spec() instead
  loader = importlib.find_loader(fullname, path)
2022-10-15 14:18:52,388	INFO worker.py:1518 -- Started a local Ray instance.
2022-10-15 14:18:56,894	WARNING unified.py:54 -- Could not instantiate JsonLogger: keys must be str, int, float, bool or None, not bytes.
2022-10-15 14:18:56,909	INFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.
2022-10-15 14:18:56,913	INFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
/global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.
  warnings.warn(
[2m[36m(RolloutWorker pid=226008)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
[2m[36m(RolloutWorker pid=226008)[0m   if not hasattr(tensorboard, "__version__") or LooseVersion(
[2m[36m(RolloutWorker pid=226009)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
[2m[36m(RolloutWorker pid=226009)[0m   if not hasattr(tensorboard, "__version__") or LooseVersion(
[2m[36m(RolloutWorker pid=226008)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/gym/core.py:200: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
[2m[36m(RolloutWorker pid=226008)[0m   deprecation(
[2m[36m(RolloutWorker pid=226009)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/gym/core.py:200: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
[2m[36m(RolloutWorker pid=226009)[0m   deprecation(
2022-10-15 14:19:10,951	WARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!
[2m[36m(RolloutWorker pid=226089)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
[2m[36m(RolloutWorker pid=226089)[0m   if not hasattr(tensorboard, "__version__") or LooseVersion(
[2m[36m(RolloutWorker pid=226090)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
[2m[36m(RolloutWorker pid=226090)[0m   if not hasattr(tensorboard, "__version__") or LooseVersion(
[2m[36m(RolloutWorker pid=226089)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/gym/core.py:200: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
[2m[36m(RolloutWorker pid=226089)[0m   deprecation(
[2m[36m(RolloutWorker pid=226090)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/gym/core.py:200: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.[0m
[2m[36m(RolloutWorker pid=226090)[0m   deprecation(
2022-10-15 14:19:22,743	INFO trainable.py:160 -- Trainable.setup took 25.840 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-10-15 14:19:22,744	WARNING util.py:65 -- Install gputil for GPU system monitoring.
[2m[36m(RolloutWorker pid=226008)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/ray/rllib/evaluation/episode.py:91: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version
[2m[36m(RolloutWorker pid=226008)[0m   self.episode_id: int = random.randrange(2e9)
[2m[36m(RolloutWorker pid=226009)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/ray/rllib/evaluation/episode.py:91: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version
[2m[36m(RolloutWorker pid=226009)[0m   self.episode_id: int = random.randrange(2e9)
[2m[36m(RolloutWorker pid=226089)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/ray/rllib/evaluation/episode.py:91: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version
[2m[36m(RolloutWorker pid=226089)[0m   self.episode_id: int = random.randrange(2e9)
[2m[36m(RolloutWorker pid=226090)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/ray/rllib/evaluation/episode.py:91: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version
[2m[36m(RolloutWorker pid=226090)[0m   self.episode_id: int = random.randrange(2e9)
[2m[36m(RolloutWorker pid=226089)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/ray/rllib/evaluation/episode.py:91: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version
[2m[36m(RolloutWorker pid=226089)[0m   self.episode_id: int = random.randrange(2e9)
[2m[36m(RolloutWorker pid=226090)[0m /global/home/users/yanlarry/.conda/envs/ActiveRL/lib/python3.10/site-packages/ray/rllib/evaluation/episode.py:91: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version
[2m[36m(RolloutWorker pid=226090)[0m   self.episode_id: int = random.randrange(2e9)
wandb: WARNING Not logging key "ray/tune/hist_stats/episode_reward". Histograms must have fewer than 512 bins
wandb: WARNING Not logging key "ray/tune/evaluation/hist_stats/episode_reward". Histograms must have fewer than 512 bins
wandb: WARNING Not logging key "ray/tune/sampler_results/hist_stats/episode_reward". Histograms must have fewer than 512 bins
['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226008)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226008)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226008)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226009)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226009)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226009)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226008)[0m FullyConnectedNetwork(['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']

[2m[36m(RolloutWorker pid=226008)[0m   (_logits): SlimFC(
[2m[36m(RolloutWorker pid=226008)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226008)[0m       (0): Linear(in_features=256, out_features=4, bias=True)
[2m[36m(RolloutWorker pid=226008)[0m     )
[2m[36m(RolloutWorker pid=226008)[0m   )
[2m[36m(RolloutWorker pid=226008)[0m   (_hidden_layers): Sequential(
[2m[36m(RolloutWorker pid=226008)[0m     (0): SlimFC(
[2m[36m(RolloutWorker pid=226008)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226008)[0m         (0): Linear(in_features=81, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226008)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226008)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226008)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226008)[0m         )
[2m[36m(RolloutWorker pid=226008)[0m       )
[2m[36m(RolloutWorker pid=226008)[0m     )
[2m[36m(RolloutWorker pid=226008)[0m     (1): SlimFC(
[2m[36m(RolloutWorker pid=226008)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226008)[0m         (0): Linear(in_features=256, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226008)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226008)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226008)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226008)[0m         )
[2m[36m(RolloutWorker pid=226008)[0m       )
[2m[36m(RolloutWorker pid=226008)[0m     )
[2m[36m(RolloutWorker pid=226008)[0m   )
[2m[36m(RolloutWorker pid=226008)[0m   (_value_branch): SlimFC(
[2m[36m(RolloutWorker pid=226008)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226008)[0m       (0): Linear(in_features=256, out_features=1, bias=True)
[2m[36m(RolloutWorker pid=226008)[0m     )
[2m[36m(RolloutWorker pid=226008)[0m   )
[2m[36m(RolloutWorker pid=226008)[0m )
[2m[36m(RolloutWorker pid=226009)[0m FullyConnectedNetwork(
[2m[36m(RolloutWorker pid=226009)[0m   (_logits): SlimFC(
[2m[36m(RolloutWorker pid=226009)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226009)[0m       (0): Linear(in_features=256, out_features=4, bias=True)
[2m[36m(RolloutWorker pid=226009)[0m     )
[2m[36m(RolloutWorker pid=226009)[0m   )
[2m[36m(RolloutWorker pid=226009)[0m   (_hidden_layers): Sequential(
[2m[36m(RolloutWorker pid=226009)[0m     (0): SlimFC(
[2m[36m(RolloutWorker pid=226009)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226009)[0m         (0): Linear(in_features=81, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226009)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226009)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226009)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226009)[0m         )
[2m[36m(RolloutWorker pid=226009)[0m       )
[2m[36m(RolloutWorker pid=226009)[0m     )
[2m[36m(RolloutWorker pid=226009)[0m     (1): SlimFC(
[2m[36m(RolloutWorker pid=226009)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226009)[0m         (0): Linear(in_features=256, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226009)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226009)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226009)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226009)[0m         )
[2m[36m(RolloutWorker pid=226009)[0m       )
[2m[36m(RolloutWorker pid=226009)[0m     )
[2m[36m(RolloutWorker pid=226009)[0m   )
[2m[36m(RolloutWorker pid=226009)[0m   (_value_branch): SlimFC(
[2m[36m(RolloutWorker pid=226009)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226009)[0m       (0): Linear(in_features=256, out_features=1, bias=True)
[2m[36m(RolloutWorker pid=226009)[0m     )
[2m[36m(RolloutWorker pid=226009)[0m   )
[2m[36m(RolloutWorker pid=226009)[0m )
FullyConnectedNetwork(
  (_logits): SlimFC(
    (_model): Sequential(
      (0): Linear(in_features=256, out_features=4, bias=True)
    )
  )
  (_hidden_layers): Sequential(
    (0): SlimFC(
      (_model): Sequential(
        (0): Linear(in_features=81, out_features=256, bias=True)
        (1): Sequential(
          (0): Tanh()
          (1): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (1): SlimFC(
      (_model): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Sequential(
          (0): Tanh()
          (1): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (_value_branch): SlimFC(
    (_model): Sequential(
      (0): Linear(in_features=256, out_features=1, bias=True)
    )
  )
)
[2m[36m(RolloutWorker pid=226089)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226089)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226090)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226090)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226089)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226090)[0m ['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
[2m[36m(RolloutWorker pid=226090)[0m FullyConnectedNetwork(
[2m[36m(RolloutWorker pid=226090)[0m   (_logits): SlimFC(
[2m[36m(RolloutWorker pid=226090)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226090)[0m       (0): Linear(in_features=256, out_features=4, bias=True)
[2m[36m(RolloutWorker pid=226090)[0m     )
[2m[36m(RolloutWorker pid=226090)[0m   )
[2m[36m(RolloutWorker pid=226090)[0m   (_hidden_layers): Sequential(
[2m[36m(RolloutWorker pid=226090)[0m     (0): SlimFC(
[2m[36m(RolloutWorker pid=226090)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226090)[0m         (0): Linear(in_features=81, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226090)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226090)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226090)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226090)[0m         )
[2m[36m(RolloutWorker pid=226090)[0m       )
[2m[36m(RolloutWorker pid=226090)[0m     )
[2m[36m(RolloutWorker pid=226090)[0m     (1): SlimFC(
[2m[36m(RolloutWorker pid=226090)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226090)[0m         (0): Linear(in_features=256, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226090)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226090)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226090)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226090)[0m         )
[2m[36m(RolloutWorker pid=226090)[0m       )
[2m[36m(RolloutWorker pid=226090)[0m     )
[2m[36m(RolloutWorker pid=226090)[0m   )
[2m[36m(RolloutWorker pid=226090)[0m   (_value_branch): SlimFC(
[2m[36m(RolloutWorker pid=226090)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226090)[0m       (0): Linear(in_features=256, out_features=1, bias=True)
[2m[36m(RolloutWorker pid=226090)[0m     )
[2m[36m(RolloutWorker pid=226090)[0m   )
[2m[36m(RolloutWorker pid=226090)[0m )
[2m[36m(RolloutWorker pid=226089)[0m FullyConnectedNetwork(
[2m[36m(RolloutWorker pid=226089)[0m   (_logits): SlimFC(
[2m[36m(RolloutWorker pid=226089)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226089)[0m       (0): Linear(in_features=256, out_features=4, bias=True)
[2m[36m(RolloutWorker pid=226089)[0m     )
[2m[36m(RolloutWorker pid=226089)[0m   )
[2m[36m(RolloutWorker pid=226089)[0m   (_hidden_layers): Sequential(
[2m[36m(RolloutWorker pid=226089)[0m     (0): SlimFC(
[2m[36m(RolloutWorker pid=226089)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226089)[0m         (0): Linear(in_features=81, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226089)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226089)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226089)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226089)[0m         )
[2m[36m(RolloutWorker pid=226089)[0m       )
[2m[36m(RolloutWorker pid=226089)[0m     )
[2m[36m(RolloutWorker pid=226089)[0m     (1): SlimFC(
[2m[36m(RolloutWorker pid=226089)[0m       (_model): Sequential(
[2m[36m(RolloutWorker pid=226089)[0m         (0): Linear(in_features=256, out_features=256, bias=True)
[2m[36m(RolloutWorker pid=226089)[0m         (1): Sequential(
[2m[36m(RolloutWorker pid=226089)[0m           (0): Tanh()
[2m[36m(RolloutWorker pid=226089)[0m           (1): Dropout(p=0.5, inplace=False)
[2m[36m(RolloutWorker pid=226089)[0m         )
[2m[36m(RolloutWorker pid=226089)[0m       )
[2m[36m(RolloutWorker pid=226089)[0m     )
[2m[36m(RolloutWorker pid=226089)[0m   )
[2m[36m(RolloutWorker pid=226089)[0m   (_value_branch): SlimFC(
[2m[36m(RolloutWorker pid=226089)[0m     (_model): Sequential(
[2m[36m(RolloutWorker pid=226089)[0m       (0): Linear(in_features=256, out_features=1, bias=True)
[2m[36m(RolloutWorker pid=226089)[0m     )
[2m[36m(RolloutWorker pid=226089)[0m   )
[2m[36m(RolloutWorker pid=226089)[0m )
{'0': 0.0, '1': 0.0, '2': 0.0, '3': 0.0, '4': 0.0, '5': 0.0, '6': 0.0, '7': 0.0, '8': 0.0, '9': 0.0, '10': 0.0, '11': 0.0, '12': 0.0, '13': 0.0, '14': 0.0, '15': 0.0, '16': 0.0, '17': 0.0, '18': 97.28, '19': -2.91, '20': 22.35, '21': 22.34, '22': 47.31, '23': 22.48, '24': -3.2, '25': 47.38, '26': 21.400000000000002, '27': 46.56, '28': 72.53, '29': 148.32, '30': 148.35000000000002, '31': 97.62, '32': 122.38999999999999, '33': -4.0200000000000005, '34': 72.22999999999999, '35': -3.9500000000000006, '36': 96.97, '37': 97.01, '38': 173.82, '39': 0.0, '40': 97.64, '41': 46.85000000000001, '42': 21.389999999999997, '43': 21.21, '44': 96.53, '45': 96.96000000000001, '46': 147.99, '47': 123.07, '48': 198.47, '49': 122.72, '50': 97.00999999999999, '51': 121.57, '52': 21.720000000000002, '53': 96.39, '54': 72.49000000000001, '55': 22.069999999999997, '56': 46.73, '57': 47.03999999999999, '58': 22.29, '59': 72.47000000000001, '60': -3.01, '61': -3.27, '62': -3.6999999999999997, '63': 0.0, '64': 0.0, '65': 0.0, '66': 0.0, '67': 0.0, '68': 0.0, '69': 0.0, '70': 0.0, '71': 0.0, '72': 0.0, '73': 0.0, '74': 0.0, '75': 0.0, '76': 0.0, '77': 0.0, '78': 0.0, '79': 0.0, '80': 0.0}
['WWWWWWWWW', 'WWWWWWWWW', 'EEEEEEEEE', 'EEEEEEEEE', 'SEEGEEEEE', 'EEEEEEEEE', 'EEEEEEEEE', 'WWWWWWWWW', 'WWWWWWWWW']
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.063 MB of 0.065 MB uploaded (0.000 MB deduped)wandb: \ 0.065 MB of 0.065 MB uploaded (0.000 MB deduped)wandb: | 0.065 MB of 0.065 MB uploaded (0.000 MB deduped)wandb: / 0.071 MB of 0.071 MB uploaded (0.000 MB deduped)wandb: - 0.009 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: \ 0.009 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: | 0.132 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: / 0.132 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: - 0.132 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: \ 0.132 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: | 0.132 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: / 0.132 MB of 0.132 MB uploaded (0.000 MB deduped)wandb: - 0.132 MB of 0.132 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                                         global_step ‚ñÅ
wandb:                                      ray/tune/agent_timesteps_total ‚ñÅ
wandb:                           ray/tune/counters/num_agent_steps_sampled ‚ñÅ
wandb:                           ray/tune/counters/num_agent_steps_trained ‚ñÅ
wandb:                             ray/tune/counters/num_env_steps_sampled ‚ñÅ
wandb:                             ray/tune/counters/num_env_steps_trained ‚ñÅ
wandb:                                                       ray/tune/done ‚ñÅ
wandb:                                           ray/tune/episode_len_mean ‚ñÅ
wandb:                                         ray/tune/episode_reward_max ‚ñÅ
wandb:                                        ray/tune/episode_reward_mean ‚ñÅ
wandb:                                         ray/tune/episode_reward_min ‚ñÅ
wandb:                                         ray/tune/episodes_this_iter ‚ñÅ
wandb:                                             ray/tune/episodes_total ‚ñÅ
wandb:                                ray/tune/evaluation/episode_len_mean ‚ñÅ
wandb:                              ray/tune/evaluation/episode_reward_max ‚ñÅ
wandb:                             ray/tune/evaluation/episode_reward_mean ‚ñÅ
wandb:                              ray/tune/evaluation/episode_reward_min ‚ñÅ
wandb:                              ray/tune/evaluation/episodes_this_iter ‚ñÅ
wandb:               ray/tune/evaluation/num_agent_steps_sampled_this_iter ‚ñÅ
wandb:                 ray/tune/evaluation/num_env_steps_sampled_this_iter ‚ñÅ
wandb:                             ray/tune/evaluation/num_faulty_episodes ‚ñÅ
wandb:                             ray/tune/evaluation/num_healthy_workers ‚ñÅ
wandb:                           ray/tune/evaluation/num_recreated_workers ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/0 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/1 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/10 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/11 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/12 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/13 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/14 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/15 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/16 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/17 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/18 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/19 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/2 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/20 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/21 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/22 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/23 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/24 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/25 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/26 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/27 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/28 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/29 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/3 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/30 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/31 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/32 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/33 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/34 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/35 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/36 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/37 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/38 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/39 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/4 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/40 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/41 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/42 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/43 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/44 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/45 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/46 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/47 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/48 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/49 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/5 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/50 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/51 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/52 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/53 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/54 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/55 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/56 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/57 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/58 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/59 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/6 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/60 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/61 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/62 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/63 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/64 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/65 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/66 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/67 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/68 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/69 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/7 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/70 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/71 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/72 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/73 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/74 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/75 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/76 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/77 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/78 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/79 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/8 ‚ñÅ
wandb:                             ray/tune/evaluation/per_cell_rewards/80 ‚ñÅ
wandb:                              ray/tune/evaluation/per_cell_rewards/9 ‚ñÅ
wandb:          ray/tune/evaluation/sampler_perf/mean_action_processing_ms ‚ñÅ
wandb:                 ray/tune/evaluation/sampler_perf/mean_env_render_ms ‚ñÅ
wandb:                   ray/tune/evaluation/sampler_perf/mean_env_wait_ms ‚ñÅ
wandb:                  ray/tune/evaluation/sampler_perf/mean_inference_ms ‚ñÅ
wandb:         ray/tune/evaluation/sampler_perf/mean_raw_obs_processing_ms ‚ñÅ
wandb:                             ray/tune/evaluation/timesteps_this_iter ‚ñÅ
wandb:     ray/tune/info/learner/default_policy/learner_stats/cur_kl_coeff ‚ñÅ
wandb:           ray/tune/info/learner/default_policy/learner_stats/cur_lr ‚ñÅ
wandb:          ray/tune/info/learner/default_policy/learner_stats/entropy ‚ñÅ
wandb:    ray/tune/info/learner/default_policy/learner_stats/entropy_coeff ‚ñÅ
wandb:               ray/tune/info/learner/default_policy/learner_stats/kl ‚ñÅ
wandb:      ray/tune/info/learner/default_policy/learner_stats/policy_loss ‚ñÅ
wandb:       ray/tune/info/learner/default_policy/learner_stats/total_loss ‚ñÅ
wandb: ray/tune/info/learner/default_policy/learner_stats/vf_explained_var ‚ñÅ
wandb:          ray/tune/info/learner/default_policy/learner_stats/vf_loss ‚ñÅ
wandb:                               ray/tune/info/num_agent_steps_sampled ‚ñÅ
wandb:                               ray/tune/info/num_agent_steps_trained ‚ñÅ
wandb:                                 ray/tune/info/num_env_steps_sampled ‚ñÅ
wandb:                                 ray/tune/info/num_env_steps_trained ‚ñÅ
wandb:                                   ray/tune/iterations_since_restore ‚ñÅ
wandb:                                    ray/tune/num_agent_steps_sampled ‚ñÅ
wandb:                                    ray/tune/num_agent_steps_trained ‚ñÅ
wandb:                                      ray/tune/num_env_steps_sampled ‚ñÅ
wandb:                            ray/tune/num_env_steps_sampled_this_iter ‚ñÅ
wandb:                                      ray/tune/num_env_steps_trained ‚ñÅ
wandb:                            ray/tune/num_env_steps_trained_this_iter ‚ñÅ
wandb:                                        ray/tune/num_faulty_episodes ‚ñÅ
wandb:                                        ray/tune/num_healthy_workers ‚ñÅ
wandb:                                      ray/tune/num_recreated_workers ‚ñÅ
wandb:                                ray/tune/num_steps_trained_this_iter ‚ñÅ
wandb:                                      ray/tune/perf/cpu_util_percent ‚ñÅ
wandb:                                      ray/tune/perf/ram_util_percent ‚ñÅ
wandb:                     ray/tune/sampler_perf/mean_action_processing_ms ‚ñÅ
wandb:                            ray/tune/sampler_perf/mean_env_render_ms ‚ñÅ
wandb:                              ray/tune/sampler_perf/mean_env_wait_ms ‚ñÅ
wandb:                             ray/tune/sampler_perf/mean_inference_ms ‚ñÅ
wandb:                    ray/tune/sampler_perf/mean_raw_obs_processing_ms ‚ñÅ
wandb:                           ray/tune/sampler_results/episode_len_mean ‚ñÅ
wandb:                         ray/tune/sampler_results/episode_reward_max ‚ñÅ
wandb:                        ray/tune/sampler_results/episode_reward_mean ‚ñÅ
wandb:                         ray/tune/sampler_results/episode_reward_min ‚ñÅ
wandb:                         ray/tune/sampler_results/episodes_this_iter ‚ñÅ
wandb:                        ray/tune/sampler_results/num_faulty_episodes ‚ñÅ
wandb:     ray/tune/sampler_results/sampler_perf/mean_action_processing_ms ‚ñÅ
wandb:            ray/tune/sampler_results/sampler_perf/mean_env_render_ms ‚ñÅ
wandb:              ray/tune/sampler_results/sampler_perf/mean_env_wait_ms ‚ñÅ
wandb:             ray/tune/sampler_results/sampler_perf/mean_inference_ms ‚ñÅ
wandb:    ray/tune/sampler_results/sampler_perf/mean_raw_obs_processing_ms ‚ñÅ
wandb:                                         ray/tune/time_since_restore ‚ñÅ
wandb:                                           ray/tune/time_this_iter_s ‚ñÅ
wandb:                                    ray/tune/timers/learn_throughput ‚ñÅ
wandb:                                       ray/tune/timers/learn_time_ms ‚ñÅ
wandb:                                     ray/tune/timers/load_throughput ‚ñÅ
wandb:                                        ray/tune/timers/load_time_ms ‚ñÅ
wandb:                               ray/tune/timers/synch_weights_time_ms ‚ñÅ
wandb:                          ray/tune/timers/training_iteration_time_ms ‚ñÅ
wandb:                                    ray/tune/timesteps_since_restore ‚ñÅ
wandb:                                            ray/tune/timesteps_total ‚ñÅ
wandb:                                                ray/tune/warmup_time ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                                         global_step 4000
wandb:                                      ray/tune/agent_timesteps_total 4000.0
wandb:                           ray/tune/counters/num_agent_steps_sampled 4000.0
wandb:                           ray/tune/counters/num_agent_steps_trained 4000.0
wandb:                             ray/tune/counters/num_env_steps_sampled 4000.0
wandb:                             ray/tune/counters/num_env_steps_trained 4000.0
wandb:                                                       ray/tune/done 0.0
wandb:                                           ray/tune/episode_len_mean 5.6169
wandb:                                         ray/tune/episode_reward_max 500.0
wandb:                                        ray/tune/episode_reward_mean 49.04845
wandb:                                         ray/tune/episode_reward_min -17.8
wandb:                                         ray/tune/episodes_this_iter 710.0
wandb:                                             ray/tune/episodes_total 710.0
wandb:                                ray/tune/evaluation/episode_len_mean 6.68642
wandb:                              ray/tune/evaluation/episode_reward_max 500.0
wandb:                             ray/tune/evaluation/episode_reward_mean 89.04173
wandb:                              ray/tune/evaluation/episode_reward_min -17.4
wandb:                              ray/tune/evaluation/episodes_this_iter 810.0
wandb:               ray/tune/evaluation/num_agent_steps_sampled_this_iter 5416.0
wandb:                 ray/tune/evaluation/num_env_steps_sampled_this_iter 5416.0
wandb:                             ray/tune/evaluation/num_faulty_episodes 0.0
wandb:                             ray/tune/evaluation/num_healthy_workers 2.0
wandb:                           ray/tune/evaluation/num_recreated_workers 0.0
wandb:                              ray/tune/evaluation/per_cell_rewards/0 0.0
wandb:                              ray/tune/evaluation/per_cell_rewards/1 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/10 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/11 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/12 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/13 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/14 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/15 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/16 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/17 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/18 22.15
wandb:                             ray/tune/evaluation/per_cell_rewards/19 71.92
wandb:                              ray/tune/evaluation/per_cell_rewards/2 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/20 47.08
wandb:                             ray/tune/evaluation/per_cell_rewards/21 72.63
wandb:                             ray/tune/evaluation/per_cell_rewards/22 22.21
wandb:                             ray/tune/evaluation/per_cell_rewards/23 47.15
wandb:                             ray/tune/evaluation/per_cell_rewards/24 22.49
wandb:                             ray/tune/evaluation/per_cell_rewards/25 -3.65
wandb:                             ray/tune/evaluation/per_cell_rewards/26 46.45
wandb:                             ray/tune/evaluation/per_cell_rewards/27 97.48
wandb:                             ray/tune/evaluation/per_cell_rewards/28 122.91
wandb:                             ray/tune/evaluation/per_cell_rewards/29 148.28
wandb:                              ray/tune/evaluation/per_cell_rewards/3 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/30 173.36
wandb:                             ray/tune/evaluation/per_cell_rewards/31 173.25999
wandb:                             ray/tune/evaluation/per_cell_rewards/32 47.13
wandb:                             ray/tune/evaluation/per_cell_rewards/33 72.43
wandb:                             ray/tune/evaluation/per_cell_rewards/34 21.7
wandb:                             ray/tune/evaluation/per_cell_rewards/35 45.97
wandb:                             ray/tune/evaluation/per_cell_rewards/36 97.53
wandb:                             ray/tune/evaluation/per_cell_rewards/37 173.73
wandb:                             ray/tune/evaluation/per_cell_rewards/38 173.5
wandb:                             ray/tune/evaluation/per_cell_rewards/39 0.0
wandb:                              ray/tune/evaluation/per_cell_rewards/4 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/40 147.58
wandb:                             ray/tune/evaluation/per_cell_rewards/41 147.91
wandb:                             ray/tune/evaluation/per_cell_rewards/42 172.89999
wandb:                             ray/tune/evaluation/per_cell_rewards/43 46.03
wandb:                             ray/tune/evaluation/per_cell_rewards/44 71.34
wandb:                             ray/tune/evaluation/per_cell_rewards/45 46.97
wandb:                             ray/tune/evaluation/per_cell_rewards/46 122.07
wandb:                             ray/tune/evaluation/per_cell_rewards/47 122.53
wandb:                             ray/tune/evaluation/per_cell_rewards/48 199.19
wandb:                             ray/tune/evaluation/per_cell_rewards/49 97.65
wandb:                              ray/tune/evaluation/per_cell_rewards/5 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/50 72.3
wandb:                             ray/tune/evaluation/per_cell_rewards/51 21.36
wandb:                             ray/tune/evaluation/per_cell_rewards/52 46.27
wandb:                             ray/tune/evaluation/per_cell_rewards/53 71.86
wandb:                             ray/tune/evaluation/per_cell_rewards/54 21.73
wandb:                             ray/tune/evaluation/per_cell_rewards/55 21.87
wandb:                             ray/tune/evaluation/per_cell_rewards/56 72.54
wandb:                             ray/tune/evaluation/per_cell_rewards/57 148.44
wandb:                             ray/tune/evaluation/per_cell_rewards/58 98.05
wandb:                             ray/tune/evaluation/per_cell_rewards/59 46.83
wandb:                              ray/tune/evaluation/per_cell_rewards/6 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/60 46.97
wandb:                             ray/tune/evaluation/per_cell_rewards/61 71.92
wandb:                             ray/tune/evaluation/per_cell_rewards/62 -3.83
wandb:                             ray/tune/evaluation/per_cell_rewards/63 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/64 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/65 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/66 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/67 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/68 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/69 0.0
wandb:                              ray/tune/evaluation/per_cell_rewards/7 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/70 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/71 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/72 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/73 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/74 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/75 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/76 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/77 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/78 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/79 0.0
wandb:                              ray/tune/evaluation/per_cell_rewards/8 0.0
wandb:                             ray/tune/evaluation/per_cell_rewards/80 0.0
wandb:                              ray/tune/evaluation/per_cell_rewards/9 0.0
wandb:          ray/tune/evaluation/sampler_perf/mean_action_processing_ms 0.0595
wandb:                 ray/tune/evaluation/sampler_perf/mean_env_render_ms 0.0
wandb:                   ray/tune/evaluation/sampler_perf/mean_env_wait_ms 0.10873
wandb:                  ray/tune/evaluation/sampler_perf/mean_inference_ms 1.67988
wandb:         ray/tune/evaluation/sampler_perf/mean_raw_obs_processing_ms 0.34964
wandb:                             ray/tune/evaluation/timesteps_this_iter 5416.0
wandb:     ray/tune/info/learner/default_policy/learner_stats/cur_kl_coeff 0.2
wandb:           ray/tune/info/learner/default_policy/learner_stats/cur_lr 5e-05
wandb:          ray/tune/info/learner/default_policy/learner_stats/entropy 1.38243
wandb:    ray/tune/info/learner/default_policy/learner_stats/entropy_coeff 0.0
wandb:               ray/tune/info/learner/default_policy/learner_stats/kl 0.00381
wandb:      ray/tune/info/learner/default_policy/learner_stats/policy_loss -0.00701
wandb:       ray/tune/info/learner/default_policy/learner_stats/total_loss 8.95611
wandb: ray/tune/info/learner/default_policy/learner_stats/vf_explained_var 1e-05
wandb:          ray/tune/info/learner/default_policy/learner_stats/vf_loss 8.96236
wandb:                               ray/tune/info/num_agent_steps_sampled 4000.0
wandb:                               ray/tune/info/num_agent_steps_trained 4000.0
wandb:                                 ray/tune/info/num_env_steps_sampled 4000.0
wandb:                                 ray/tune/info/num_env_steps_trained 4000.0
wandb:                                   ray/tune/iterations_since_restore 1.0
wandb:                                    ray/tune/num_agent_steps_sampled 4000.0
wandb:                                    ray/tune/num_agent_steps_trained 4000.0
wandb:                                      ray/tune/num_env_steps_sampled 4000.0
wandb:                            ray/tune/num_env_steps_sampled_this_iter 4000.0
wandb:                                      ray/tune/num_env_steps_trained 4000.0
wandb:                            ray/tune/num_env_steps_trained_this_iter 4000.0
wandb:                                        ray/tune/num_faulty_episodes 0.0
wandb:                                        ray/tune/num_healthy_workers 2.0
wandb:                                      ray/tune/num_recreated_workers 0.0
wandb:                                ray/tune/num_steps_trained_this_iter 4000.0
wandb:                                      ray/tune/perf/cpu_util_percent 60.26182
wandb:                                      ray/tune/perf/ram_util_percent 25.85515
wandb:                     ray/tune/sampler_perf/mean_action_processing_ms 0.06399
wandb:                            ray/tune/sampler_perf/mean_env_render_ms 0.0
wandb:                              ray/tune/sampler_perf/mean_env_wait_ms 0.11588
wandb:                             ray/tune/sampler_perf/mean_inference_ms 1.88332
wandb:                    ray/tune/sampler_perf/mean_raw_obs_processing_ms 45.32281
wandb:                           ray/tune/sampler_results/episode_len_mean 5.6169
wandb:                         ray/tune/sampler_results/episode_reward_max 500.0
wandb:                        ray/tune/sampler_results/episode_reward_mean 49.04845
wandb:                         ray/tune/sampler_results/episode_reward_min -17.8
wandb:                         ray/tune/sampler_results/episodes_this_iter 710.0
wandb:                        ray/tune/sampler_results/num_faulty_episodes 0.0
wandb:     ray/tune/sampler_results/sampler_perf/mean_action_processing_ms 0.06399
wandb:            ray/tune/sampler_results/sampler_perf/mean_env_render_ms 0.0
wandb:              ray/tune/sampler_results/sampler_perf/mean_env_wait_ms 0.11588
wandb:             ray/tune/sampler_results/sampler_perf/mean_inference_ms 1.88332
wandb:    ray/tune/sampler_results/sampler_perf/mean_raw_obs_processing_ms 45.32281
wandb:                                         ray/tune/time_since_restore 115.03045
wandb:                                           ray/tune/time_this_iter_s 115.03045
wandb:                                    ray/tune/timers/learn_throughput 682.62299
wandb:                                       ray/tune/timers/learn_time_ms 5859.74805
wandb:                                     ray/tune/timers/load_throughput 455419.96875
wandb:                                        ray/tune/timers/load_time_ms 8.783
wandb:                               ray/tune/timers/synch_weights_time_ms 3.101
wandb:                          ray/tune/timers/training_iteration_time_ms 105588.5625
wandb:                                    ray/tune/timesteps_since_restore 0.0
wandb:                                            ray/tune/timesteps_total 4000.0
wandb:                                                ray/tune/warmup_time 25.84176
wandb: 
wandb: Synced sweet-deluge-138: https://wandb.ai/social-game-rl/active-rl/runs/gaohb5qn
wandb: Synced 6 W&B file(s), 2 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20221015_141843-gaohb5qn/logs
